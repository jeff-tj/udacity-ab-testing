---
title: "ab_testing"
author: "Jeff Tjeuw"
date: "29/08/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Udacity AB Testing Project
Here we consider a hypothetical experiment run by Udacity: some students who clicked the "Start Free Trial" button would see a question asking how much time they had available to devote, with students who indicated 5 or less hours receiving a message that a greater time commitment would be required.

The hypothesis being tested: would this reduce the number of frustrated students leaving the free trial.

## Metric Choice
Given the hypothesis we would want to see the proportion of students who make a payment increase.

### Invariant Metrics:
* Number of Cookies
* Number of Clicks
* Click-through-probability
We should expect these to remain unchanged as users at these points would not have experienced the change - i.e. we would expect roughly the same number of students to visit the site as well as the same number of students to click the button. Therefore the click-through-probability should also be invariant.

### Evaluation Metrics:
For evaluation metrics we have a number of choices. The number of user-ids would be a poor choice - as the change in an of itself would not tell us whether those students continued past the free-trial or not. Therefore, our invariant metrics could be:
* Gross conversion
* Retention
* Net conversion

## Measuring Standard Deviation
We calculate the standard deviation of our evaluation metrics.

```{r introduction}
setwd("~/data_research/u_ab_testing")
baseline = read.csv("baseline.csv", header = FALSE)
click_no = 3200
enrol_no = 660
# gross conversion = users who enrol / users click start button
gross_conv = 	0.20625

# net conversion = users who pay / users click start button
net_conv = 0.1093125

# retention = users who pay / users who enroll
ret_rate = 0.53


# Estimate the standard deviation using binomial/normal equation
estimate_sd <- function(prob, n) {
  return(sqrt(prob*(1-prob)/n))
}

gross_conv_sd <- estimate_sd(gross_conv, click_no)
net_conv_sd <- estimate_sd(net_conv, click_no)
ret_sd <- estimate_sd(ret_rate, enrol_no)

# Calculate some rates
base_total = 40000
click_rate = click_no / base_total
enrol_rate = enrol_no / base_total

# Scale the analytic standard deviations by sample size
sample_n = 5000
sgc_sd <- gross_conv_sd * sqrt(click_no/(click_rate * sample_n))
snc_sd <- net_conv_sd * sqrt(click_no/(click_rate * sample_n))
sr_sd <- ret_sd * sqrt(enrol_no/(enrol_rate * sample_n))

# Print out our results
sprintf("Gross conversion sd: %.4f", sgc_sd)
sprintf("Net conversion sd: %.4f", snc_sd)
sprintf("Retention sd: %.4f", sr_sd)
```

## Sizing

```{r pageviews, echo=FALSE}
# Adapt the udacity code to calculate experiment size
# Link: https://goo.gl/T5jBPi
sample_size <- function(se, d_min, alpha=0.05, beta=0.2, n_max=40000) {
  # se - standard error at n=1
  # d_min - minimum detecetable effect
  # alpha - confidence interval
  # beta - (1-beta) is the probability the effect detected
  # n_max - the largest number to test
  z_star = -qnorm(alpha / 2)
  n = 1
  while (n<=n_max) {
    prob = pnorm(z_star*se/sqrt(n), mean=d_min, sd=(se/sqrt(n)))
    if (prob <= beta) {
      return(n)
    }
    n = n + 1
  }
  return("No answer with current n_max")
}

# Calculate sample size for retention
ret_size <- sample_size(se=sqrt(ret_rate*(1 - ret_rate)*2), d_min=0.01)
sprintf("Sample size required for retention: %i", ret_size)
sprintf("Pageviews required: %i", ceiling(ret_size * 2 / enrol_rate))

# Calculate sample size for conversion
conv_size <- sample_size(se=sqrt(net_conv*(1 - net_conv)*2), d_min=0.0075)
sprintf("Sample size required for conversion: %i", conv_size)
sprintf("Pageviews required: %i", ceiling(conv_size * 2 /click_rate))
```
Note this is different to the answers given by the web based application for the answer used for the grader (http://www.evanmiller.org/ab-testing/sample-size.html).
The answers given are the same as given by the sample code linked above.

## Sanity Checks on the Data

```{r sanity checks}
# Read in the experimental data
res_con <- read.csv("final_proj_results_control.csv", header = TRUE)
res_exp <- read.csv("final_proj_results_exp.csv", header = TRUE)

# Omit the rows where we do not have complete data
res_con_no_na <- na.omit(res_con)
res_exp_no_na <- na.omit(res_exp)

# Count pageviews and analyse
pv_con <- colSums(res_con[2])
pv_exp <- colSums(res_exp[2])
pv_frac = pv_exp / (pv_con + pv_exp)
pv_sd = sqrt(0.5 * 0.5 / (pv_con + pv_exp))
pv_z = 1.96
pv_cil = 0.5 - pv_z * pv_sd
pv_ciu = 0.5 + pv_z * pv_sd
sprintf("Pageview confidence interval is (%.4f, %.4f)", pv_cil, pv_ciu)
sprintf("Pageview fraction in experiment group: %.4f", pv_frac)

# Count clicks and analyse
ck_con <- colSums(res_con[3])
ck_exp <- colSums(res_exp[3])
ck_frac = ck_exp / (ck_con + ck_exp)
ck_sd = sqrt(0.5 * 0.5 / (ck_con + ck_exp))
ck_z = 1.96
ck_cil = 0.5 - ck_z * ck_sd
ck_ciu = 0.5 + ck_z * ck_sd
sprintf("Clicks confidence interval is (%.4f, %.4f)", ck_cil, ck_ciu)
sprintf("Clicks fraction in experiment group: %.4f", ck_frac)
```
Both invariant metrics pass the sanity check - namely the fraction in each one is within the 95% confidence interval.
(Note however that the fraction required by the grader seems to be the fraction in the control group)
